from __future__ import annotations

from typing import List, Dict, Tuple, Any
import asyncio
import json
import re
import os
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
import streamlit as st

from .models import MenuItem

# LangChain v1ç³»ã§ output_parsers ã®å ´æ‰€ãŒå‰²ã‚Œã‚‹ã®ã§ã€ã“ã“ã¯ classic ã«å›ºå®šã—ã¦å®‰å®šåŒ–
from langchain_classic.output_parsers import StructuredOutputParser, ResponseSchema


# ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
response_schemas = [
    ResponseSchema(name="menu_title", description="ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã‚¿ã‚¤ãƒˆãƒ«"),
    ResponseSchema(name="menu_content", description="ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®èª¬æ˜æ–‡")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

# --------------------------------------------------------------------
# 1) ä¸è¦éƒ¨åˆ†å‰Šé™¤ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
cleanup_template = """
å¤–å›½äººè¦³å…‰å®¢å‘ã‘ã«ã€ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç¿»è¨³ã‚’è¡Œã„ã¾ã™ã€‚
å‰æº–å‚™ã¨ã—ã¦ã€ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ä¸è¦ãªè‡ªå·±ã‚¢ãƒ”ãƒ¼ãƒ«ã‚„é ‘å¼µã‚Šã«é–¢ã™ã‚‹è¨€è‘‰ãªã©ã‚’å‰Šé™¤ã—ã€æ–™ç†ã®èª¬æ˜ã‚„æ­´å²ãƒ»é£Ÿã¹æ–¹ãªã©åˆ©ç”¨è€…ã«æœ‰ç›Šãªæƒ…å ±ã¯æ®‹ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€æ–‡åŒ–ã‚„æ­´å²çš„ãªèƒŒæ™¯æƒ…å ±ãŒå¿…è¦ãªæƒ…å ±ãŒã‚ã‚Œã°ã€å†…å®¹ã®ä¸­ã«é©å®œè¿½åŠ ã—ã¦ãã ã•ã„ã€‚

{format_instructions}

ã€åŸæ–‡ã€‘
{original_text}

ã€ä¸è¦éƒ¨åˆ†å‰Šé™¤å¾Œã€‘
"""

cleanup_prompt = PromptTemplate(
    input_variables=["original_text"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=cleanup_template
)

# --------------------------------------------------------------------
# 2) æ—¥æœ¬èª â†’ è‹±èªç¿»è¨³ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
ja_to_en_template = """
å¤–å›½äººè¦³å…‰å®¢å‘ã‘ã«ã€ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è‡ªç„¶ãªè‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

{format_instructions}

ã€æ—¥æœ¬èªã€‘
{cleaned_japanese_text}

ã€è‹±èªè¨³ã€‘
"""

ja_to_en_prompt = PromptTemplate(
    input_variables=["cleaned_japanese_text"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=ja_to_en_template
)

# --------------------------------------------------------------------
# 3) è‹±èª â†’ å¤šè¨€èªç¿»è¨³ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
# --------------------------------------------------------------------
# 3) è‹±èª â†’ å¤šè¨€èªç¿»è¨³ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
multi_trans_template = """
ä»¥ä¸‹ã®è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’ {target_language} ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

{persona_instruction}

{format_instructions}

ã€è‹±èªåŸæ–‡ã€‘
{english_text}

ã€{target_language}è¨³ã€‘
"""

multi_trans_prompt = PromptTemplate(
    input_variables=["english_text", "target_language", "persona_instruction"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=multi_trans_template
)

# ãƒšãƒ«ã‚½ãƒŠå®šç¾©ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã¨å…±é€šåŒ–æ¤œè¨ã ãŒã€ä¸€æ—¦ã“ã“ã«å®šç¾©ï¼‰
PERSONA_PROMPTS = {
    "æ±äº¬ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼é¢¨ (è‰¶ã‚„ã‹)": "Translate in a sophisticated, alluring, and rich tone, similar to high-end lifestyle magazines (like Tokyo Calendar). Use evocative and emotional language.",
    "å±…é…’å±‹ã®å¤§å°†é¢¨ (å…ƒæ°—)": "Translate in a friendly, energetic, and casual tone, like a lively Izakaya owner. Use punchy and welcoming language.",
    "é«˜ç´šæ–™äº­é¢¨ (å³æ ¼)": "Translate in a highly formal, polite, and respectful tone, typical of a luxury Ryotei. Use elegant and traditional phrasing.",
    "æ¨™æº– (ä¸å¯§)": "Translate in a standard, polite, and clear tone.",
}

# ãƒšãƒ«ã‚½ãƒŠå®šç¾©ï¼ˆãƒ¡ã‚¤ãƒ³ã‚¢ãƒ—ãƒªã¨å…±é€šåŒ–æ¤œè¨ã ãŒã€ä¸€æ—¦ã“ã“ã«å®šç¾©ï¼‰
PERSONA_PROMPTS = {
    "æ±äº¬ã‚«ãƒ¬ãƒ³ãƒ€ãƒ¼é¢¨ (è‰¶ã‚„ã‹)": "Translate in a sophisticated, alluring, and rich tone, similar to high-end lifestyle magazines (like Tokyo Calendar). Use evocative and emotional language.",
    "å±…é…’å±‹ã®å¤§å°†é¢¨ (å…ƒæ°—)": "Translate in a friendly, energetic, and casual tone, like a lively Izakaya owner. Use punchy and welcoming language.",
    "é«˜ç´šæ–™äº­é¢¨ (å³æ ¼)": "Translate in a highly formal, polite, and respectful tone, typical of a luxury Ryotei. Use elegant and traditional phrasing.",
    "æ¨™æº– (ä¸å¯§)": "Translate in a standard, polite, and clear tone.",
}

# ç°¡æ˜“ã‚³ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ« (Gemini 2.5 Flash / 1.5 Flash è¿‘ä¼¼å€¤)
# $0.075 / 1M tokens (Input)
# $0.30 / 1M tokens (Output)
# 1 USD = 150 JPY
COST_MODEL = {
    "input_price_per_1m_usd": 0.075,
    "output_price_per_1m_usd": 0.30,
    "usd_jpy_rate": 150.0
}

def log_api_usage(phase: str, model: str, tokens_in: int, tokens_out: int, store_id: str = "TRIAL_USER"):
    """APIåˆ©ç”¨ãƒ­ã‚°ã‚’ logs/api_usage_log.csv ã«è¿½è¨˜ã™ã‚‹"""
    try:
        # ã‚³ã‚¹ãƒˆè¨ˆç®—
        cost_usd = (tokens_in / 1_000_000 * COST_MODEL["input_price_per_1m_usd"]) + \
                   (tokens_out / 1_000_000 * COST_MODEL["output_price_per_1m_usd"])
        cost_jpy = cost_usd * COST_MODEL["usd_jpy_rate"]
        
        # ãƒ­ã‚°ãƒ•ã‚¡ã‚¤ãƒ«ã¸ã®æ›¸ãè¾¼ã¿
        log_path = "logs/api_usage_log.csv"
        # ãƒ˜ãƒƒãƒ€ãƒ¼æ›¸ãè¾¼ã¿ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºã®å ´åˆï¼‰
        if not os.path.exists(log_path) or os.stat(log_path).st_size == 0:
            with open(log_path, "w", encoding="utf-8") as f:
                f.write("timestamp,store_id,phase,model,tokens_in,tokens_out,cost_jpy\n")
        
        from datetime import datetime
        now = datetime.now().isoformat()
        
        with open(log_path, "a", encoding="utf-8") as f:
            f.write(f"{now},{store_id},{phase},{model},{tokens_in},{tokens_out},{cost_jpy:.4f}\n")
            
    except Exception as e:
        print(f"Log Error: {e}") # ãƒ­ã‚°å¤±æ•—ã§ãƒ¡ã‚¤ãƒ³å‡¦ç†ã‚’æ­¢ã‚ãªã„

def get_llm(api_key: str, temperature: float = 0.0):
    return ChatGoogleGenerativeAI(
        model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash"),
        google_api_key=api_key,
        temperature=temperature,
    )

def remove_unnecessary_parts(text_list: List[MenuItem], api_key: str) -> List[MenuItem]:
    """1ä»¶ãšã¤ä¸è¦éƒ¨åˆ†å‰Šé™¤ã‚’è¡Œã„ã€çµæœã‚’MenuItemã®ãƒªã‚¹ãƒˆã§è¿”ã™"""
    llm = get_llm(api_key)
    # chain = cleanup_prompt | llm | output_parser # æ—§å®Ÿè£…
    # UsageMetadataã‚’å–å¾—ã™ã‚‹ãŸã‚ã« chain ã‚’åˆ†å‰²å®Ÿè¡Œã™ã‚‹
    
    results = []
    progress_text = "âœ’ï¸ æ—¥æœ¬èªæ ¡æ­£"
    my_bar = st.progress(0, text=progress_text)
    total_items = len(text_list)
    
    for i, menu_item in enumerate(text_list, 1):
        try:
            input_text = {
                "menu_title": menu_item.menu_title,
                "menu_content": menu_item.menu_content
            }
            
            # æ‰‹å‹•ã§Chainã‚’å®Ÿè¡Œã—ã¦Metadataã‚’æŠœã
            formatted_prompt = cleanup_prompt.format_prompt(original_text=json.dumps(input_text, ensure_ascii=False))
            response = llm.invoke(formatted_prompt)
            
            # ãƒ­ã‚°è¨˜éŒ²
            if hasattr(response, "response_metadata") and "token_usage" in response.response_metadata:
                usage = response.response_metadata["token_usage"]
                prompt_tokens = usage.get("prompt_tokens", 0) or usage.get("total_tokens", 0) # Fallback if needed
                completion_tokens = usage.get("completion_tokens", 0)
                # Gemini 2.5 Flashã¯ response_metadata ã®æ§‹é€ ãŒå°‘ã—é•ã†å ´åˆãŒã‚ã‚‹ãŒã€
                # langchain-google-genai ã§ã¯é€šå¸¸ 'token_usage': {'prompt_tokens': X, 'completion_tokens': Y, 'total_tokens': Z}
                log_api_usage("cleanup_ja", llm.model, prompt_tokens, completion_tokens)
            
            parsed_output = output_parser.parse(response.content)
            
            new_item = MenuItem(
                menu_title=parsed_output["menu_title"],
                menu_content=parsed_output["menu_content"]
            )
            results.append(new_item)
            
            progress = int(i / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({i}/{total_items})")
            
        except Exception as e:
            st.error(f"æ—¥æœ¬èªæ ¡æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            results.append(MenuItem.create_error(str(e)))
    
    my_bar.progress(100, text=f"âœ… æ—¥æœ¬èªæ ¡æ­£å®Œäº†")
    return results

def translate_japanese_to_english(menu_items: List[MenuItem], api_key: str, persona: str = "æ¨™æº– (ä¸å¯§)") -> List[MenuItem]:
    """æ—¥æœ¬èªã®MenuItemãƒªã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³ã—ã€çµæœã‚’MenuItemã®ãƒªã‚¹ãƒˆã§è¿”ã™"""
    llm = get_llm(api_key)
    
    # è‹±èªç¿»è¨³ç”¨ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆã«ã‚‚ãƒšãƒ«ã‚½ãƒŠé©ç”¨
    ja_to_en_template_persona = """
    å¤–å›½äººè¦³å…‰å®¢å‘ã‘ã«ã€ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è‡ªç„¶ãªè‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚
    
    {persona_instruction}
    
    {format_instructions}
    
    ã€æ—¥æœ¬èªã€‘
    {cleaned_japanese_text}
    
    ã€è‹±èªè¨³ã€‘
    """
    
    ja_to_en_prompt_persona = PromptTemplate(
        input_variables=["cleaned_japanese_text", "persona_instruction"],
        partial_variables={"format_instructions": output_parser.get_format_instructions()},
        template=ja_to_en_template_persona
    )

    # chain = ja_to_en_prompt_persona | llm | output_parser
    
    results = []
    progress_text = "ğŸ”¤ è‹±èªç¿»è¨³"
    my_bar = st.progress(0, text=progress_text)
    total_items = len(menu_items)
    
    persona_instruction = PERSONA_PROMPTS.get(persona, PERSONA_PROMPTS["æ¨™æº– (ä¸å¯§)"])

    for i, menu_item in enumerate(menu_items, 1):
        try:
            input_text = {
                "menu_title": menu_item.menu_title,
                "menu_content": menu_item.menu_content
            }
            
            formatted_prompt = ja_to_en_prompt_persona.format_prompt(
                cleaned_japanese_text=json.dumps(input_text, ensure_ascii=False),
                persona_instruction=persona_instruction
            )
            response = llm.invoke(formatted_prompt)

            # ãƒ­ã‚°è¨˜éŒ²
            if hasattr(response, "response_metadata") and "token_usage" in response.response_metadata:
                usage = response.response_metadata["token_usage"]
                log_api_usage("trans_en", llm.model, usage.get("prompt_tokens", 0), usage.get("completion_tokens", 0))

            parsed_output = output_parser.parse(response.content)
            
            translated_item = MenuItem(
                menu_title=parsed_output["menu_title"],
                menu_content=parsed_output["menu_content"]
            )
            results.append(translated_item)
            
            progress = int(i / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({i}/{total_items})")
            
        except Exception as e:
            st.error(f"è‹±èªç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            results.append(MenuItem.create_error(str(e)))
    
    my_bar.progress(100, text=f"âœ… è‹±èªç¿»è¨³å®Œäº†")
    return results

async def translate_english_to_many_async(menu_items: List[MenuItem], target_languages: Dict[str, List[MenuItem]], api_key: str, persona: str = "æ¨™æº– (ä¸å¯§)") -> Dict[str, List[MenuItem]]:
    """è‹±èªã‹ã‚‰æŒ‡å®šè¨€èªã¸ã®ç¿»è¨³ã‚’éåŒæœŸã§ä¸¦åˆ—å®Ÿè¡Œ"""
    llm = get_llm(api_key)
    # chain = multi_trans_prompt | llm | output_parser
    
    error_messages = []
    rate_limit_status = {"is_waiting": False}
    
    persona_instruction = PERSONA_PROMPTS.get(persona, PERSONA_PROMPTS["æ¨™æº– (ä¸å¯§)"])

    async def translate_with_retry(input_dict: dict, lang: str, max_retries: int = 5, initial_wait: float = 10.0) -> dict:
        wait_time = initial_wait
        for attempt in range(max_retries):
            try:
                if rate_limit_status["is_waiting"]:
                    await asyncio.sleep(1)
                
                formatted_prompt = multi_trans_prompt.format_prompt(
                    english_text=json.dumps(input_dict, ensure_ascii=False),
                    target_language=lang,
                    persona_instruction=persona_instruction
                )
                
                # ainvokeã§responseã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã‚’å–å¾—ã™ã‚‹ã®ã¯é›£ã—ã„(chainãªã®ã§)ã€‚
                # llm.ainvokeã‚’ä½¿ã†å½¢ã«æ›¸ãæ›ãˆã‚‹å¿…è¦ãŒã‚ã‚‹ãŒã€
                # langchain chainã§metadataã‚’å–ã‚‹ã«ã¯ callbacks ã‚’ä½¿ã†ã®ãŒå®šçŸ³ã ãŒã€
                # ã“ã“ã§ã¯ã‚·ãƒ³ãƒ—ãƒ«ã« llm.ainvoke + parser.parse ã«æ›¸ãæ›ãˆã‚‹
                response = await llm.ainvoke(formatted_prompt)
                
                # ãƒ­ã‚°è¨˜éŒ²
                if hasattr(response, "response_metadata") and "token_usage" in response.response_metadata:
                    usage = response.response_metadata["token_usage"]
                    # asyncãƒãƒƒãƒå†…ã§IOã™ã‚‹ã®ã§ãƒ–ãƒ­ãƒƒã‚­ãƒ³ã‚°æ³¨æ„ã ãŒã€ãƒ­ã‚°æ›¸ãè¾¼ã¿ã¯é«˜é€Ÿã¨ä»®å®š
                    log_api_usage(f"trans_{lang}", llm.model, usage.get("prompt_tokens", 0), usage.get("completion_tokens", 0))

                return output_parser.parse(response.content)

            except Exception as e:
                error_msg = str(e).lower()
                if "rate_limit" in error_msg and attempt < max_retries - 1:
                    if not rate_limit_status["is_waiting"]:
                        rate_limit_status["is_waiting"] = True
                        with st.status(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…æ©Ÿä¸­({int(wait_time)}ç§’)...") as status:
                            await asyncio.sleep(wait_time)
                            status.update(label="âœ… å†é–‹ã—ã¾ã™")
                        rate_limit_status["is_waiting"] = False
                    wait_time *= 2
                    continue
                raise e

    async def translate_menu_item(menu_item: MenuItem, lang: str) -> Tuple[str, MenuItem]:
        try:
            input_text = {"menu_title": menu_item.menu_title, "menu_content": menu_item.menu_content}
            parsed_output = await translate_with_retry(input_text, lang)
            return lang, MenuItem(menu_title=parsed_output["menu_title"], menu_content=parsed_output["menu_content"])
        except Exception as e:
            error_messages.append(f"ğŸš« {lang}ã®ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            return lang, MenuItem.create_error(str(e))

    async def translate_language(lang: str) -> Tuple[str, List[MenuItem]]:
        progress_text = f"ğŸ”„ {lang}ã®ç¿»è¨³"
        my_bar = st.progress(0, text=progress_text)
        
        batch_size = 3 # ä¸¦åˆ—æ•°ã‚’å°‘ã—æŠ‘ãˆã¦å®‰å®šã•ã›ã‚‹
        tasks = [translate_menu_item(item, lang) for item in menu_items]
        translated_items = []
        total_items = len(tasks)
        
        for i in range(0, total_items, batch_size):
            batch = tasks[i:i + batch_size]
            batch_results = await asyncio.gather(*batch)
            translated_items.extend(batch_results)
            progress = int(min((i + batch_size), total_items) / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({min(i + batch_size, total_items)}/{total_items})")
        
        my_bar.progress(100, text=f"âœ… {lang}ã®ç¿»è¨³å®Œäº†")
        return lang, [item[1] for item in translated_items]

    translation_tasks = [translate_language(lang) for lang in target_languages.keys()]
    translation_results = await asyncio.gather(*translation_tasks)
    
    results = dict(translation_results)
    if error_messages:
        with st.expander("âš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
            for msg in error_messages:
                st.error(msg)
    return results