from __future__ import annotations

from typing import List, Dict, Tuple, Any
import asyncio
import json
import re
import os
from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
import streamlit as st

from .models import MenuItem

# LangChain v1Á≥ª„Åß output_parsers „ÅÆÂ†¥ÊâÄ„ÅåÂâ≤„Çå„Çã„ÅÆ„Åß„ÄÅ„Åì„Åì„ÅØ classic „Å´Âõ∫ÂÆö„Åó„Å¶ÂÆâÂÆöÂåñ
from langchain_classic.output_parsers import StructuredOutputParser, ResponseSchema


# „Çπ„Ç≠„Éº„Éû„ÅÆÂÆöÁæ©
response_schemas = [
    ResponseSchema(name="menu_title", description="„É°„Éã„É•„Éº„ÅÆ„Çø„Ç§„Éà„É´"),
    ResponseSchema(name="menu_content", description="„É°„Éã„É•„Éº„ÅÆË™¨ÊòéÊñá")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

# --------------------------------------------------------------------
# 1) ‰∏çË¶ÅÈÉ®ÂàÜÂâäÈô§„ÅÆ„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
# --------------------------------------------------------------------
cleanup_template = """
Â§ñÂõΩ‰∫∫Ë¶≥ÂÖâÂÆ¢Âêë„Åë„Å´„ÄÅ„É¨„Çπ„Éà„É©„É≥„ÅÆ„É°„Éã„É•„Éº„ÅÆÁøªË®≥„ÇíË°å„ÅÑ„Åæ„Åô„ÄÇ
ÂâçÊ∫ñÂÇô„Å®„Åó„Å¶„ÄÅ‰ª•‰∏ã„ÅÆÊó•Êú¨Ë™û„ÉÜ„Ç≠„Çπ„Éà„Åã„Çâ„ÄÅ‰∏çË¶Å„Å™Ëá™Â∑±„Ç¢„Éî„Éº„É´„ÇÑÈ†ëÂºµ„Çä„Å´Èñ¢„Åô„ÇãË®ÄËëâ„Å™„Å©„ÇíÂâäÈô§„Åó„ÄÅÊñôÁêÜ„ÅÆË™¨Êòé„ÇÑÊ≠¥Âè≤„ÉªÈ£ü„ÅπÊñπ„Å™„Å©Âà©Áî®ËÄÖ„Å´ÊúâÁõä„Å™ÊÉÖÂ†±„ÅØÊÆã„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
„Åæ„Åü„ÄÅÊñáÂåñ„ÇÑÊ≠¥Âè≤ÁöÑ„Å™ËÉåÊôØÊÉÖÂ†±„ÅåÂøÖË¶Å„Å™ÊÉÖÂ†±„Åå„ÅÇ„Çå„Å∞„ÄÅÂÜÖÂÆπ„ÅÆ‰∏≠„Å´ÈÅ©ÂÆúËøΩÂä†„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

{format_instructions}

„ÄêÂéüÊñá„Äë
{original_text}

„Äê‰∏çË¶ÅÈÉ®ÂàÜÂâäÈô§Âæå„Äë
"""

cleanup_prompt = PromptTemplate(
    input_variables=["original_text"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=cleanup_template
)

# --------------------------------------------------------------------
# 2) Êó•Êú¨Ë™û ‚Üí Ëã±Ë™ûÁøªË®≥„ÅÆ„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
# --------------------------------------------------------------------
ja_to_en_template = """
Â§ñÂõΩ‰∫∫Ë¶≥ÂÖâÂÆ¢Âêë„Åë„Å´„ÄÅ‰ª•‰∏ã„ÅÆÊó•Êú¨Ë™û„É°„Éã„É•„Éº„ÇíËá™ÁÑ∂„Å™Ëã±Ë™û„Å´ÁøªË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

{format_instructions}

„ÄêÊó•Êú¨Ë™û„Äë
{cleaned_japanese_text}

„ÄêËã±Ë™ûË®≥„Äë
"""

ja_to_en_prompt = PromptTemplate(
    input_variables=["cleaned_japanese_text"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=ja_to_en_template
)

# --------------------------------------------------------------------
# 3) Ëã±Ë™û ‚Üí Â§öË®ÄË™ûÁøªË®≥„ÅÆ„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
# --------------------------------------------------------------------
# --------------------------------------------------------------------
# 3) Ëã±Ë™û ‚Üí Â§öË®ÄË™ûÁøªË®≥„ÅÆ„Åü„ÇÅ„ÅÆ„Éó„É≠„É≥„Éó„Éà
# --------------------------------------------------------------------
multi_trans_template = """
‰ª•‰∏ã„ÅÆËã±Ë™û„ÉÜ„Ç≠„Çπ„Éà„Çí {target_language} „Å´ÁøªË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ

{persona_instruction}

{format_instructions}

„ÄêËã±Ë™ûÂéüÊñá„Äë
{english_text}

„Äê{target_language}Ë®≥„Äë
"""

multi_trans_prompt = PromptTemplate(
    input_variables=["english_text", "target_language", "persona_instruction"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=multi_trans_template
)

# „Éö„É´„ÇΩ„ÉäÂÆöÁæ©Ôºà„É°„Ç§„É≥„Ç¢„Éó„É™„Å®ÂÖ±ÈÄöÂåñÊ§úË®é„Å†„Åå„ÄÅ‰∏ÄÊó¶„Åì„Åì„Å´ÂÆöÁæ©Ôºâ
PERSONA_PROMPTS = {
    "Êù±‰∫¨„Ç´„É¨„É≥„ÉÄ„ÉºÈ¢® (Ëâ∂„ÇÑ„Åã)": "Translate in a sophisticated, alluring, and rich tone, similar to high-end lifestyle magazines (like Tokyo Calendar). Use evocative and emotional language.",
    "Â±ÖÈÖíÂ±ã„ÅÆÂ§ßÂ∞ÜÈ¢® (ÂÖÉÊ∞ó)": "Translate in a friendly, energetic, and casual tone, like a lively Izakaya owner. Use punchy and welcoming language.",
    "È´òÁ¥öÊñô‰∫≠È¢® (Âé≥Ê†º)": "Translate in a highly formal, polite, and respectful tone, typical of a luxury Ryotei. Use elegant and traditional phrasing.",
    "Ê®ôÊ∫ñ (‰∏ÅÂØß)": "Translate in a standard, polite, and clear tone.",
}

# „Éö„É´„ÇΩ„ÉäÂÆöÁæ©Ôºà„É°„Ç§„É≥„Ç¢„Éó„É™„Å®ÂÖ±ÈÄöÂåñÊ§úË®é„Å†„Åå„ÄÅ‰∏ÄÊó¶„Åì„Åì„Å´ÂÆöÁæ©Ôºâ
PERSONA_PROMPTS = {
    "Êù±‰∫¨„Ç´„É¨„É≥„ÉÄ„ÉºÈ¢® (Ëâ∂„ÇÑ„Åã)": "Translate in a sophisticated, alluring, and rich tone, similar to high-end lifestyle magazines (like Tokyo Calendar). Use evocative and emotional language.",
    "Â±ÖÈÖíÂ±ã„ÅÆÂ§ßÂ∞ÜÈ¢® (ÂÖÉÊ∞ó)": "Translate in a friendly, energetic, and casual tone, like a lively Izakaya owner. Use punchy and welcoming language.",
    "È´òÁ¥öÊñô‰∫≠È¢® (Âé≥Ê†º)": "Translate in a highly formal, polite, and respectful tone, typical of a luxury Ryotei. Use elegant and traditional phrasing.",
    "Ê®ôÊ∫ñ (‰∏ÅÂØß)": "Translate in a standard, polite, and clear tone.",
}

from .observability import log_api_cost

# Removed local COST_MODEL and log_api_usage integration
# Uses centralized observability module now.

# Ë®ÄË™ûÂà•„É≠„Éº„Ç´„É©„Ç§„Ç∫„Éö„É´„ÇΩ„Éä (Transcreation Prompts)
# ÂêÑË®ÄË™û„ÅÆÊñáÂåñËÉåÊôØ„Å´Âêà„Çè„Åõ„Åü„Äå„É©„Ç§„Çø„Éº‰∫∫Ê†º„Äç„ÇíÂÆöÁæ©
LANGUAGE_PERSONAS = {
    "English": "You are a friendly and knowledgeable 'New Yorker Foodie'. Use appetizing adjectives (crispy, savory, zest) and a welcoming tone suitable for a hip casual dining guide.",
    "Chinese": "You are a passionate 'Gourmand from Shanghai'. Use poetic and evocative four-character idioms where appropriate to describe texture and flavor depth. Tone should be enthusiastic and respectful.",
    "Korean": "You are a trend-conscious 'Seoul Influencer'. Use modern, punchy vocabulary that appeals to young travelers. Emphasize visual and flavor impact (spicy, chewy, refreshing).",
    "French": "You are an elegant 'Parisian Sommelier'. Focus on the harmony of flavors, ingredients, and potential pairings. Use sophisticated and descriptive language.",
    "Italian": "You are a cheerful 'Trattoria Owner'. Use warm, passionate language. Emphasize freshness of ingredients and the joy of eating (Buono!).",
    "Spanish": "You are a friendly 'Tapas Bar Host'. Use lively, inviting language. Invite the guest to enjoy a shared experience.",
    "Thai": "You are a helpful 'Local Food Guide'. Explain flavors clearly (sour, sweet, spicy) with a gentle, smiling tone.",
    "German": "You are a reliable 'Gourmet Critic'. Be descriptive, precise, and emphasize quality and craftsmanship of the dish.",
    "Vietnamese": "You are a 'Street Food Connoisseur'. Use warm, appetizing language that evokes the aroma and freshness of herbs and spices.",
    "Indonesian": "You are a 'Friendly Local Host'. Use warm, inviting language consistent with Indonesian hospitality.",
    "Taiwanese": "You are a 'Taipei Food Blogger'. Use traditional characters. Tone is friendly, relatable, and emphasizes 'Q-texture' and authentic flavors.",
    "HongKong": "You are a 'Cantonese Chef'. Use Cantonese stylistic nuances (written in Traditional Chinese). Emphasize 'Wok Hei' and freshness.",
}

def get_llm(api_key: str, temperature: float = 0.0):
    return ChatGoogleGenerativeAI(
        model=os.getenv("GEMINI_MODEL", "gemini-2.5-flash"),
        google_api_key=api_key,
        temperature=temperature,
    )

def remove_unnecessary_parts(text_list: List[MenuItem], api_key: str) -> List[MenuItem]:
    """1‰ª∂„Åö„Å§‰∏çË¶ÅÈÉ®ÂàÜÂâäÈô§„ÇíË°å„ÅÑ„ÄÅÁµêÊûú„ÇíMenuItem„ÅÆ„É™„Çπ„Éà„ÅßËøî„Åô"""
    llm = get_llm(api_key)
    # chain = cleanup_prompt | llm | output_parser # ÊóßÂÆüË£Ö
    # UsageMetadata„ÇíÂèñÂæó„Åô„Çã„Åü„ÇÅ„Å´ chain „ÇíÂàÜÂâ≤ÂÆüË°å„Åô„Çã
    
    results = []
    progress_text = "‚úíÔ∏è Êó•Êú¨Ë™ûÊ†°Ê≠£"
    my_bar = st.progress(0, text=progress_text)
    total_items = len(text_list)
    
    for i, menu_item in enumerate(text_list, 1):
        try:
            input_text = {
                "menu_title": menu_item.menu_title,
                "menu_content": menu_item.menu_content
            }
            
            # ÊâãÂãï„ÅßChain„ÇíÂÆüË°å„Åó„Å¶Metadata„ÇíÊäú„Åè
            formatted_prompt = cleanup_prompt.format_prompt(original_text=json.dumps(input_text, ensure_ascii=False))
            response = llm.invoke(formatted_prompt)
            
            # „É≠„Ç∞Ë®òÈå≤
            if hasattr(response, "response_metadata") and "token_usage" in response.response_metadata:
                usage = response.response_metadata["token_usage"]
                prompt_tokens = usage.get("prompt_tokens", 0) or usage.get("total_tokens", 0) # Fallback if needed
                completion_tokens = usage.get("completion_tokens", 0)
                # Gemini 2.5 Flash„ÅØ response_metadata „ÅÆÊßãÈÄ†„ÅåÂ∞ë„ÅóÈÅï„ÅÜÂ†¥Âêà„Åå„ÅÇ„Çã„Åå„ÄÅ
                # langchain-google-genai „Åß„ÅØÈÄöÂ∏∏ 'token_usage': {'prompt_tokens': X, 'completion_tokens': Y, 'total_tokens': Z}
                log_api_usage("cleanup_ja", llm.model, prompt_tokens, completion_tokens)
            
            parsed_output = output_parser.parse(response.content)
            
            new_item = MenuItem(
                menu_title=parsed_output["menu_title"],
                menu_content=parsed_output["menu_content"]
            )
            results.append(new_item)
            
            progress = int(i / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({i}/{total_items})")
            
        except Exception as e:
            st.error(f"Êó•Êú¨Ë™ûÊ†°Ê≠£‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}")
            results.append(MenuItem.create_error(str(e)))
    
    my_bar.progress(100, text=f"‚úÖ Êó•Êú¨Ë™ûÊ†°Ê≠£ÂÆå‰∫Ü")
    return results

def translate_japanese_to_english(menu_items: List[MenuItem], api_key: str, persona: str = "Ê®ôÊ∫ñ (‰∏ÅÂØß)") -> List[MenuItem]:
    """Êó•Êú¨Ë™û„ÅÆMenuItem„É™„Çπ„Éà„ÇíËã±Ë™û„Å´ÁøªË®≥„Åó„ÄÅÁµêÊûú„ÇíMenuItem„ÅÆ„É™„Çπ„Éà„ÅßËøî„Åô"""
    llm = get_llm(api_key)
    
    # Ëã±Ë™ûÁøªË®≥Áî®„Éó„É≠„É≥„Éó„Éà„Å´„ÇÇ„Éö„É´„ÇΩ„ÉäÈÅ©Áî®
    ja_to_en_template_persona = """
    Â§ñÂõΩ‰∫∫Ë¶≥ÂÖâÂÆ¢Âêë„Åë„Å´„ÄÅ‰ª•‰∏ã„ÅÆÊó•Êú¨Ë™û„É°„Éã„É•„Éº„ÇíËá™ÁÑ∂„Å™Ëã±Ë™û„Å´ÁøªË®≥„Åó„Å¶„Åè„Å†„Åï„ÅÑ„ÄÇ
    
    {persona_instruction}
    
    {format_instructions}
    
    „ÄêÊó•Êú¨Ë™û„Äë
    {cleaned_japanese_text}
    
    „ÄêËã±Ë™ûË®≥„Äë
    """
    
    ja_to_en_prompt_persona = PromptTemplate(
        input_variables=["cleaned_japanese_text", "persona_instruction"],
        partial_variables={"format_instructions": output_parser.get_format_instructions()},
        template=ja_to_en_template_persona
    )

    # chain = ja_to_en_prompt_persona | llm | output_parser
    
    results = []
    progress_text = "üî§ Ëã±Ë™ûÁøªË®≥"
    my_bar = st.progress(0, text=progress_text)
    total_items = len(menu_items)
    
    persona_instruction = PERSONA_PROMPTS.get(persona, PERSONA_PROMPTS["Ê®ôÊ∫ñ (‰∏ÅÂØß)"])

    for i, menu_item in enumerate(menu_items, 1):
        try:
            input_text = {
                "menu_title": menu_item.menu_title,
                "menu_content": menu_item.menu_content
            }
            
            formatted_prompt = ja_to_en_prompt_persona.format_prompt(
                cleaned_japanese_text=json.dumps(input_text, ensure_ascii=False),
                persona_instruction=persona_instruction
            )
            response = llm.invoke(formatted_prompt)

            # „É≠„Ç∞Ë®òÈå≤
            if hasattr(response, "response_metadata") and "token_usage" in response.response_metadata:
                usage = response.response_metadata["token_usage"]
                log_api_usage("trans_en", llm.model, usage.get("prompt_tokens", 0), usage.get("completion_tokens", 0))

            parsed_output = output_parser.parse(response.content)
            
            translated_item = MenuItem(
                menu_title=parsed_output["menu_title"],
                menu_content=parsed_output["menu_content"]
            )
            results.append(translated_item)
            
            progress = int(i / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({i}/{total_items})")
            
        except Exception as e:
            st.error(f"Ëã±Ë™ûÁøªË®≥‰∏≠„Å´„Ç®„É©„Éº„ÅåÁô∫Áîü„Åó„Åæ„Åó„Åü: {e}")
            results.append(MenuItem.create_error(str(e)))
    
    my_bar.progress(100, text=f"‚úÖ Ëã±Ë™ûÁøªË®≥ÂÆå‰∫Ü")
    return results

async def translate_english_to_many_async(menu_items: List[MenuItem], target_languages: Dict[str, List[MenuItem]], api_key: str, persona: str = "Ê®ôÊ∫ñ (‰∏ÅÂØß)") -> Dict[str, List[MenuItem]]:
    """Ëã±Ë™û„Åã„ÇâÊåáÂÆöË®ÄË™û„Å∏„ÅÆÁøªË®≥„ÇíÈùûÂêåÊúü„Åß‰∏¶ÂàóÂÆüË°å (S1-04 Transcreation Engine)"""
    llm = get_llm(api_key)
    
    # --- S1-04 Transcreation Prompt Template ---
    transcreation_template = """
    [ROLE]
    You are a transcreation copywriter for restaurant menus.
    Your voice MUST match the Persona below.

    [PERSONA]
    - Language: {target_language}
    - Speaker: {persona_role}
    - Tone: {persona_tone}
    - Forbidden: {persona_forbidden}

    [INPUT]
    - Item name (JP): {name_ja}
    - Item description (JP): {desc_ja}
    - Context: {persona}

    [OUTPUT RULES]
    1) Title format: "{Localized name}" (Keep it native script only unless specified)
    2) Body: ~18 seconds silent reading (3-beat structure: Texture/Ratio -> How to Eat -> Pairing).
    3) No medical/health claims. No ‚Äúguarantee‚Äù. No unverifiable origin claims.
    4) Be specific without inventing facts. If unknown, phrase as suggestion, not assertion.
    5) JSON Output ONLY.

    [DELIVER]
    Return JSON:
    {{
      "name": "...",
      "description": "...",
      "pairing": "..."
    }}
    """
    
    transcreation_prompt = PromptTemplate(
        input_variables=["target_language", "persona_role", "persona_tone", "persona_forbidden", "name_ja", "desc_ja", "persona"],
        template=transcreation_template
    )

    from .personas import PERSONA_DEFINITIONS, DEFAULT_QC_RULES
    from .models import MenuItem

    async def verify_quality(original_input: dict, generated_output: dict, lang: str) -> Tuple[bool, str]:
        """S1-06 QC Audit"""
        qc_prompt = f"""
        Act as a Quality Control Auditor for restaurant menu translations.
        
        [Rules]
        {DEFAULT_QC_RULES}
        
        [Source (JP)]
        Name: {original_input['menu_title']}
        Desc: {original_input['menu_content']}
        
        [Generated ({lang})]
        Name: {generated_output.get('name', 'N/A')}
        Desc: {generated_output.get('description', 'N/A')}
        Pairing: {generated_output.get('pairing', 'N/A')}
        
        Evaluate PASS or FAIL. If FAIL, provide brief reason.
        Format: "PASS" or "FAIL: [Reason]"
        """
        try:
            res = await llm.ainvoke(qc_prompt)
            content = res.content.strip()
            
            # Log QC Cost
            if hasattr(res, "response_metadata") and "token_usage" in res.response_metadata:
                usage = res.response_metadata["token_usage"]
                log_api_cost(f"QC_{lang}", llm.model, usage.get("prompt_tokens", 0), usage.get("completion_tokens", 0))

            if content.upper().startswith("PASS"):
                return True, ""
            return False, content
        except Exception as e:
            print(f"QC Error: {e}")
            return True, "" # Fail open

    async def translate_with_retry(input_dict: dict, lang: str, max_retries: int = 1) -> dict:
        # Get Persona Data (S1-03)
        persona_def = PERSONA_DEFINITIONS.get(lang, {
            "role": "Professional Translator",
            "tone": "Polite, accurate.",
            "forbidden": "Mistranslations",
            "keywords": []
        })

        for attempt in range(max_retries + 1): # Attempt 0 + Max Retries
            # 1. Generate
            formatted_prompt = transcreation_prompt.format_prompt(
                target_language=lang,
                persona_role=persona_def["role"],
                persona_tone=persona_def["tone"],
                persona_forbidden=persona_def["forbidden"],
                name_ja=input_dict["menu_title"],
                desc_ja=input_dict["menu_content"],
                persona=persona # Extra context
            )
            
            try:
                response = await llm.ainvoke(formatted_prompt)
                
                # Log Gen Cost
                if hasattr(response, "response_metadata") and "token_usage" in response.response_metadata:
                    usage = response.response_metadata["token_usage"]
                    log_api_cost(f"trans_{lang}", llm.model, usage.get("prompt_tokens", 0), usage.get("completion_tokens", 0))

                # Parse JSON
                parsed = output_parser.parse(response.content)
                
                # Normalize keys
                if "menu_title" in parsed and "name" not in parsed:
                    parsed["name"] = parsed["menu_title"]
                if "menu_content" in parsed and "description" not in parsed:
                    parsed["description"] = parsed["menu_content"]
                
                # 2. Quality Control (QC)
                is_pass, reason = await verify_quality(input_dict, parsed, lang)
                if is_pass:
                    print(f"‚úÖ {lang}: Pass")
                    return parsed
                else:
                    print(f"‚ö†Ô∏è {lang}: QC Fail - {reason} (Attempt {attempt+1})")
                    continue 

            except Exception as e:
                # print(f"Error {lang}: {e}")
                pass
            
            # Wait briefly before retry if not last attempt
            if attempt < max_retries:
                await asyncio.sleep(1)
        
        # Fallback if all attempts fail
        return {
             "name": input_dict["menu_title"], 
             "description": f"(Translation Pending) {input_dict['menu_content']}",
             "pairing": ""
        }

    async def process_single_item(item: MenuItem, lang: str) -> MenuItem:
        try:
            input_data = {"menu_title": item.menu_title, "menu_content": item.menu_content}
            result_dict = await translate_with_retry(input_data, lang, max_retries=1)
            
            return MenuItem(
                menu_title=result_dict.get("name", item.menu_title),
                menu_content=result_dict.get("description", item.menu_content),
                pairing=result_dict.get("pairing", ""),
                confidence=0.9,
                status="confirmed"
            )
        except Exception as e:
            return MenuItem.create_error(f"{lang} Error: {str(e)}")

    # --- Main Loop ---
    results = {lang: [] for lang in target_languages.keys()}
    
    # Process per language to manage tasks clearly
    for lang in target_languages.keys():
        lang_tasks = []
        for item in menu_items:
            lang_tasks.append(process_single_item(item, lang))
        
        # Gather results for this language
        # (For 14 languages * N items, this is somewhat heavy, but async handles it well enough for <100 total reqs)
        # Using a progress bar per language is good UX
        
        # Note: We can't easily update a Streamlit progress bar from inside async gather without callbacks.
        # So we just await all.
        
        lang_results = await asyncio.gather(*lang_tasks)
        results[lang] = list(lang_results)
        
    return results