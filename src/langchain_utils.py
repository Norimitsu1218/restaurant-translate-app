# langchain_utils.py

from langchain_core.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
import streamlit as st
import json
from typing import List, Dict, Tuple
import asyncio
from .models import MenuItem
from langchain_classic.output_parsers import StructuredOutputParser, ResponseSchema

# ã‚¹ã‚­ãƒ¼ãƒã®å®šç¾©
response_schemas = [
    ResponseSchema(name="menu_title", description="ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ã‚¿ã‚¤ãƒˆãƒ«"),
    ResponseSchema(name="menu_content", description="ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®èª¬æ˜æ–‡")
]
output_parser = StructuredOutputParser.from_response_schemas(response_schemas)

# --------------------------------------------------------------------
# 1) ä¸è¦éƒ¨åˆ†å‰Šé™¤ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
cleanup_template = """
å¤–å›½äººè¦³å…‰å®¢å‘ã‘ã«ã€ãƒ¬ã‚¹ãƒˆãƒ©ãƒ³ã®ãƒ¡ãƒ‹ãƒ¥ãƒ¼ã®ç¿»è¨³ã‚’è¡Œã„ã¾ã™ã€‚
å‰æº–å‚™ã¨ã—ã¦ã€ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ†ã‚­ã‚¹ãƒˆã‹ã‚‰ã€ä¸è¦ãªè‡ªå·±ã‚¢ãƒ”ãƒ¼ãƒ«ã‚„é ‘å¼µã‚Šã«é–¢ã™ã‚‹è¨€è‘‰ãªã©ã‚’å‰Šé™¤ã—ã€æ–™ç†ã®èª¬æ˜ã‚„æ­´å²ãƒ»é£Ÿã¹æ–¹ãªã©åˆ©ç”¨è€…ã«æœ‰ç›Šãªæƒ…å ±ã¯æ®‹ã—ã¦ãã ã•ã„ã€‚
ã¾ãŸã€æ–‡åŒ–ã‚„æ­´å²çš„ãªèƒŒæ™¯æƒ…å ±ãŒå¿…è¦ãªæƒ…å ±ãŒã‚ã‚Œã°ã€å†…å®¹ã®ä¸­ã«é©å®œè¿½åŠ ã—ã¦ãã ã•ã„ã€‚

{format_instructions}

ã€åŸæ–‡ã€‘
{original_text}

ã€ä¸è¦éƒ¨åˆ†å‰Šé™¤å¾Œã€‘
"""

cleanup_prompt = PromptTemplate(
    input_variables=["original_text"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=cleanup_template
)

# --------------------------------------------------------------------
# 2) æ—¥æœ¬èª â†’ è‹±èªç¿»è¨³ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
ja_to_en_template = """
å¤–å›½äººè¦³å…‰å®¢å‘ã‘ã«ã€ä»¥ä¸‹ã®æ—¥æœ¬èªãƒ¡ãƒ‹ãƒ¥ãƒ¼ã‚’è‡ªç„¶ãªè‹±èªã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

{format_instructions}

ã€æ—¥æœ¬èªã€‘
{cleaned_japanese_text}

ã€è‹±èªè¨³ã€‘
"""

ja_to_en_prompt = PromptTemplate(
    input_variables=["cleaned_japanese_text"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=ja_to_en_template
)

# --------------------------------------------------------------------
# 3) è‹±èª â†’ å¤šè¨€èªç¿»è¨³ã®ãŸã‚ã®ãƒ—ãƒ­ãƒ³ãƒ—ãƒˆ
# --------------------------------------------------------------------
multi_trans_template = """
ä»¥ä¸‹ã®è‹±èªãƒ†ã‚­ã‚¹ãƒˆã‚’ {target_language} ã«ç¿»è¨³ã—ã¦ãã ã•ã„ã€‚

{format_instructions}

ã€è‹±èªåŸæ–‡ã€‘
{english_text}

ã€{target_language}è¨³ã€‘
"""

multi_trans_prompt = PromptTemplate(
    input_variables=["english_text", "target_language"],
    partial_variables={"format_instructions": output_parser.get_format_instructions()},
    template=multi_trans_template
)

def get_llm(api_key: str, temperature: float = 0.0):
    return ChatGoogleGenerativeAI(
        model="gemini-1.5-flash",
        google_api_key=api_key,
        temperature=temperature,
    )

def remove_unnecessary_parts(text_list: List[MenuItem], api_key: str) -> List[MenuItem]:
    """1ä»¶ãšã¤ä¸è¦éƒ¨åˆ†å‰Šé™¤ã‚’è¡Œã„ã€çµæœã‚’MenuItemã®ãƒªã‚¹ãƒˆã§è¿”ã™"""
    llm = get_llm(api_key)
    chain = cleanup_prompt | llm | output_parser
    
    results = []
    progress_text = "âœ’ï¸ æ—¥æœ¬èªæ ¡æ­£"
    my_bar = st.progress(0, text=progress_text)
    total_items = len(text_list)
    
    for i, menu_item in enumerate(text_list, 1):
        try:
            input_text = {
                "menu_title": menu_item.menu_title,
                "menu_content": menu_item.menu_content
            }
            # LCEL invoke
            parsed_output = chain.invoke({"original_text": json.dumps(input_text, ensure_ascii=False)})
            
            new_item = MenuItem(
                menu_title=parsed_output["menu_title"],
                menu_content=parsed_output["menu_content"]
            )
            results.append(new_item)
            
            progress = int(i / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({i}/{total_items})")
            
        except Exception as e:
            st.error(f"æ—¥æœ¬èªæ ¡æ­£ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            results.append(MenuItem.create_error(str(e)))
    
    my_bar.progress(100, text=f"âœ… æ—¥æœ¬èªæ ¡æ­£å®Œäº†")
    return results

def translate_japanese_to_english(menu_items: List[MenuItem], api_key: str) -> List[MenuItem]:
    """æ—¥æœ¬èªã®MenuItemãƒªã‚¹ãƒˆã‚’è‹±èªã«ç¿»è¨³ã—ã€çµæœã‚’MenuItemã®ãƒªã‚¹ãƒˆã§è¿”ã™"""
    llm = get_llm(api_key)
    chain = ja_to_en_prompt | llm | output_parser
    
    results = []
    progress_text = "ğŸ”¤ è‹±èªç¿»è¨³"
    my_bar = st.progress(0, text=progress_text)
    total_items = len(menu_items)
    
    for i, menu_item in enumerate(menu_items, 1):
        try:
            input_text = {
                "menu_title": menu_item.menu_title,
                "menu_content": menu_item.menu_content
            }
            # LCEL invoke
            parsed_output = chain.invoke({"cleaned_japanese_text": json.dumps(input_text, ensure_ascii=False)})
            
            translated_item = MenuItem(
                menu_title=parsed_output["menu_title"],
                menu_content=parsed_output["menu_content"]
            )
            results.append(translated_item)
            
            progress = int(i / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({i}/{total_items})")
            
        except Exception as e:
            st.error(f"è‹±èªç¿»è¨³ä¸­ã«ã‚¨ãƒ©ãƒ¼ãŒç™ºç”Ÿã—ã¾ã—ãŸ: {e}")
            results.append(MenuItem.create_error(str(e)))
    
    my_bar.progress(100, text=f"âœ… è‹±èªç¿»è¨³å®Œäº†")
    return results

async def translate_english_to_many_async(menu_items: List[MenuItem], target_languages: Dict[str, List[MenuItem]], api_key: str) -> Dict[str, List[MenuItem]]:
    """è‹±èªã‹ã‚‰æŒ‡å®šè¨€èªã¸ã®ç¿»è¨³ã‚’éåŒæœŸã§ä¸¦åˆ—å®Ÿè¡Œ"""
    llm = get_llm(api_key)
    chain = multi_trans_prompt | llm | output_parser
    
    error_messages = []
    rate_limit_status = {"is_waiting": False}
    
    async def translate_with_retry(input_dict: dict, lang: str, max_retries: int = 5, initial_wait: float = 10.0) -> dict:
        wait_time = initial_wait
        for attempt in range(max_retries):
            try:
                if rate_limit_status["is_waiting"]:
                    await asyncio.sleep(1)
                # LCEL ainvoke
                return await chain.ainvoke({
                    "english_text": json.dumps(input_dict, ensure_ascii=False),
                    "target_language": lang
                })
            except Exception as e:
                error_msg = str(e).lower()
                if "rate_limit" in error_msg and attempt < max_retries - 1:
                    if not rate_limit_status["is_waiting"]:
                        rate_limit_status["is_waiting"] = True
                        with st.status(f"â³ ãƒ¬ãƒ¼ãƒˆåˆ¶é™å¾…æ©Ÿä¸­({int(wait_time)}ç§’)...") as status:
                            await asyncio.sleep(wait_time)
                            status.update(label="âœ… å†é–‹ã—ã¾ã™")
                        rate_limit_status["is_waiting"] = False
                    wait_time *= 2
                    continue
                raise e

    async def translate_menu_item(menu_item: MenuItem, lang: str) -> Tuple[str, MenuItem]:
        try:
            input_text = {"menu_title": menu_item.menu_title, "menu_content": menu_item.menu_content}
            parsed_output = await translate_with_retry(input_text, lang)
            return lang, MenuItem(menu_title=parsed_output["menu_title"], menu_content=parsed_output["menu_content"])
        except Exception as e:
            error_messages.append(f"ğŸš« {lang}ã®ç¿»è¨³ã‚¨ãƒ©ãƒ¼: {e}")
            return lang, MenuItem.create_error(str(e))

    async def translate_language(lang: str) -> Tuple[str, List[MenuItem]]:
        progress_text = f"ğŸ”„ {lang}ã®ç¿»è¨³"
        my_bar = st.progress(0, text=progress_text)
        
        batch_size = 3 # ä¸¦åˆ—æ•°ã‚’å°‘ã—æŠ‘ãˆã¦å®‰å®šã•ã›ã‚‹
        tasks = [translate_menu_item(item, lang) for item in menu_items]
        translated_items = []
        total_items = len(tasks)
        
        for i in range(0, total_items, batch_size):
            batch = tasks[i:i + batch_size]
            batch_results = await asyncio.gather(*batch)
            translated_items.extend(batch_results)
            progress = int(min((i + batch_size), total_items) / total_items * 100)
            my_bar.progress(progress, text=f"{progress_text} ({min(i + batch_size, total_items)}/{total_items})")
        
        my_bar.progress(100, text=f"âœ… {lang}ã®ç¿»è¨³å®Œäº†")
        return lang, [item[1] for item in translated_items]

    translation_tasks = [translate_language(lang) for lang in target_languages.keys()]
    translation_results = await asyncio.gather(*translation_tasks)
    
    results = dict(translation_results)
    if error_messages:
        with st.expander("âš ï¸ ã‚¨ãƒ©ãƒ¼è©³ç´°", expanded=False):
            for msg in error_messages:
                st.error(msg)
    return results